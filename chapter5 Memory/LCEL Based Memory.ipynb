{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LCEL 메모리**\n",
    " LangChain Explicit Learning Memory의 약자로, LangChain에서 대화 내역을 효율적으로 관리하고 학습하는 데 사용되는 메모리 시스템 중 하나입니다. 이 메모리는 주로 **명시적 학습(Explicit Learning)** 을 기반으로 대화 데이터를 처리\n",
    "\n",
    "* LCEL 메모리는 대화 중 발생한 정보를 명시적으로 저장하고, 이를 기반으로 사용자가 제공한 정보나 문맥을 학습합니다.\n",
    "* 대화가 진행될수록 사용자가 언급한 데이터를 추적하고, 필요한 경우 이 데이터를 요약하거나 호출하여 적절한 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatOpenAI 초기화**\n",
    "* 결과의 결정적 특성을 높이기 위해 '온도=0.1'을 설정하여 OpenAI 모델을 초기화합니다.\n",
    "\n",
    "**대화요약버퍼메모리**\n",
    "* 대화 내용을 메모리를 통해 저장하고 요약합니다.\n",
    "* 효율성을 보장하기 위해 토큰 크기를 제한합니다.\n",
    "\n",
    "**ChatPromptTemplate**\n",
    "* 대화의 구조를 정의합니다.\n",
    "* 과거 대화 내용을 담고 사용자 질문을 처리하도록 설계되었습니다.\n",
    "\n",
    "**load_memory 함수**\n",
    "* 메모리에서 대화 기록을 로드합니다.\n",
    "* 대화 기록을 체인에 전달합니다.\n",
    "\n",
    "**체인 구성** \n",
    "* RunnablePassthrough를 사용하여 대화 기록을 \"history\" 변수에 전달합니다.\n",
    "* 프롬프트와 모델을 결합하여 입력을 기반으로 응답을 생성합니다.\n",
    "\n",
    "**invoke_chain 함수**\n",
    "* 사용자의 질문을 입력으로 받아 체인을 실행합니다.\n",
    "* 생성된 응답을 메모리에 저장합니다.\n",
    "* 입력된 질문과 답변을 모두 저장합니다.\n",
    "* 검증을 위해 결과를 인쇄합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 LangChain 구성요소를 임포트\n",
    "from langchain.memory import ConversationSummaryBufferMemory  # 대화 내역을 요약하여 관리하는 메모리 구성요소\n",
    "from langchain.chat_models import ChatOpenAI  # OpenAI의 채팅 모델을 사용해 응답 생성\n",
    "from langchain.schema.runnable import RunnablePassthrough  # 체인을 연결하기 위한 단순 실행기\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  # 프롬프트 관련 유틸리티\n",
    "\n",
    "# 낮은 온도로 OpenAI 채팅 모델 초기화 (결과를 더 일관되게 생성)\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 대화 내용을 저장하고 요약하는 메모리 객체 정의\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,  # 요약을 위해 동일한 언어 모델 사용\n",
    "    max_token_limit=120,  # 최대 토큰 크기를 120으로 제한\n",
    "    return_messages=True,  # 요약 대신 대화 메시지 형식으로 반환\n",
    ")\n",
    "\n",
    "# 대화의 구조를 정의하는 프롬프트 객체 생성\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),  # 시스템 메시지로 AI의 역할을 정의\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 이전 대화 기록을 가져올 자리 표시자\n",
    "        (\"human\", \"{question}\"),  # 사용자의 질문이 들어갈 자리 표시자\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 메모리에서 대화 기록을 로드하는 함수 정의\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]  # 메모리 변수에서 \"history\"를 반환\n",
    "\n",
    "# 체인 정의\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(history=load_memory)  # \"history\" 변수에 로드된 대화 기록을 전달\n",
    "    | prompt  # 프롬프트와 결합\n",
    "    | llm  # OpenAI 모델에 전달하여 응답 생성\n",
    ")\n",
    "\n",
    "# 체인을 실행하고 결과를 저장하는 함수 정의\n",
    "def invoke_chain(question):\n",
    "    # 체인을 실행하여 질문에 대한 응답 생성\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # 생성된 응답을 메모리에 저장\n",
    "    memory.save_context(\n",
    "        {\"input\": question},  # 사용자의 입력 저장\n",
    "        {\"output\": result.content},  # 모델의 응답 저장\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Mo! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Mo.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
