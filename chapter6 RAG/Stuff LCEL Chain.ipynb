{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Victory Mansions is a place where Winston lives. It is a run-down apartment building with small, shabby units. The building is in disrepair, with faulty plumbing and electricity. The atmosphere is grim and oppressive, reflecting the overall bleakness of the society in which Winston resides.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# ChatOpenAI 모델을 생성합니다. 'temperature'는 모델의 응답 다양성을 조절합니다.\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 낮은 값으로 설정하여 응답의 일관성을 유지\n",
    ")\n",
    "\n",
    "# 로컬 캐시 저장소를 생성합니다. 캐시는 임베딩 저장을 위한 디렉토리를 정의합니다.\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 텍스트를 분할하기 위한 TextSplitter를 생성합니다.\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",  # 줄바꿈 문자로 텍스트를 분할\n",
    "    chunk_size=600,  # 각 청크의 최대 문자 수\n",
    "    chunk_overlap=100,  # 청크 간 중복 문자 수\n",
    ")\n",
    "\n",
    "# 텍스트 파일을 로드합니다. 여기서는 \"chapter_one.txt\" 파일을 로드합니다.\n",
    "loader = UnstructuredFileLoader(\"./document.txt\")\n",
    "\n",
    "# 로드한 파일을 정의된 TextSplitter를 사용하여 분할합니다.\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# OpenAI 임베딩 모델을 초기화합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐시를 지원하는 임베딩 객체를 생성합니다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "# 분할된 문서를 기반으로 FAISS 벡터 스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# 벡터 스토어를 검색 도구로 변환합니다.\n",
    "# chain의 가장 중요한 구성요소\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "# 채팅 프롬프트 템플릿을 정의합니다. 컨텍스트 기반 응답을 생성하도록 구성합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),  # 사용자 질문을 동적으로 삽입\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 최종 질의응답 체인을 구성합니다.\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,  # 컨텍스트를 검색합니다.\n",
    "        \"question\": RunnablePassthrough(),  # 질문을 그대로 전달합니다.\n",
    "    }\n",
    "    | prompt  # 프롬프트 템플릿에 데이터를 전달합니다.\n",
    "    | llm  # LLM(ChatOpenAI)을 사용하여 응답을 생성합니다.\n",
    ")\n",
    "\n",
    "# 체인을 실행하여 질문에 답변을 생성합니다.\n",
    "response = chain.invoke(\"Describe Victory Mansions\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
